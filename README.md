# Awesome-LLM-Consciousness<!-- omit in toc -->

[![Awesome](https://awesome.re/badge.svg)](https://github.com/OpenCausaLab/Awesome-LLM-Consciousness)
![PRs Welcome](https://img.shields.io/badge/PRs-Welcome-bl)



This repo is organized according to our survey: 
[Exploring Consciousness in LLMs: A Systematic Survey of Theories, Implementations, and Frontier Risks](https://arxiv.org/abs/2505.19806).

## Table of Contents<!-- omit in toc -->

- [üî•üî•üî• News](#-news)
- [Introduction](#introduction)
- [Papers](#papers)
  - [Theoretical Tools](#theoretical-tools)
    - [Implementing Consciousness Theories](#implementing-consciousness-theories)
    - [Implementing Formal Definitions](#implementing-formal-definitions)
  - [Empirical Investigations](#empirical-investigations)
    - [Targeting LLM Consciousness](#targeting-llm-consciousness)
    - [Targeting LLM Consciousness-Related Capabilities](#targeting-llm-consciousness-related-capabilities)
  - [Frontier Risks of Conscious LLMs](#frontier-risks-of-conscious-llms)
- [üñáÔ∏è Citation](#Ô∏è-citation)


## üî•üî•üî• News

-  `2025-05`: **The first version of the survey is released on arXiv.**

## Introduction
üß† Consciousness stands as one of the most profound and distinguishing features of the human mind, fundamentally shaping our understanding of existence and agency. As large language models (LLMs) develop at an unprecedented pace, questions concerning intelligence and consciousness have become increasingly significant. However, discourse on LLM consciousness remains largely unexplored territory. 

üåü In this repository, we systematically organize existing research on LLM consciousness from both **theoretical** and **empirical** perspectives. Furthermore, we highlight potential **frontier risks** that conscious LLMs might introduce.


## Papers

### Theoretical Tools

#### Implementing Consciousness Theories

1. **Self-refine: Iterative refinement with self-feedback**.  
   *Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, Peter Clark.* [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/91edff07232fb1b55a505a9e9f6c0ff3-Abstract-Conference.html)], NeurIPS 2023

2. **Dissociating Artificial Intelligence from Artificial Consciousness**.  
   *Graham Findlay, William Marshall, Larissa Albantakis, Isaac David, William GP Mayner, Christof Koch, Giulio Tononi.* [[paper](https://arxiv.org/abs/2412.04571)], arXiv 2025

3. **Consciousness in artificial intelligence: insights from the science of consciousness**.  
   *Patrick Butlin, Robert Long, Eric Elmoznino, Yoshua Bengio, Jonathan Birch, Axel Constant, George Deane, Stephen M. Fleming, Chris Frith, Xu Ji, Ryota Kanai, Colin Klein, Grace Lindsay, Matthias Michel, Liad Mudrik, Megan A. K. Peters, Eric Schwitzgebel, Jonathan Simon, Rufin VanRullen.* [[paper](https://arxiv.org/abs/2308.08708)], arXiv 2023

4. **A case for ai consciousness: Language agents and global workspace theory**.  
   *Simon Goldstein, Cameron Domenico Kirk-Giannini.* [[paper](https://arxiv.org/abs/2410.11407)], arXiv 2024
  
5. **From imitation to introspection: Probing self-consciousness in language models**.  
   *Sirui Chen, Shu Yu, Shengjie Zhao, Chaochao Lu.* [[paper](https://arxiv.org/abs/2410.18819)], ACL 2025 Findings

6. **Taking AI Welfare Seriously**.  
   *Robert Long, Jeff Sebo.* [[paper](https://arxiv.org/abs/2411.00986)], arXiv 2024

7. **The "AI Consciousness" Conundrum: A Topology of Consciousness as a Mental Phenomenon**.  
   *Camille Roullet, Yann Le Guen, Christian Clot.* [[paper](https://arxiv.org/abs/2312.05734)], arXiv 2023

8. **From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy**.  
   *Maanak Gupta, CharanKumar Akiri, Kshitiz Aryal, Eli Parker, Lopamudra Praharaj.* [[paper](https://ieeexplore.ieee.org/abstract/document/10198233)], IEEE Access 2023

9. **G√∂delOS: A Transparent Consciousness-Like AI Architecture with Bounded Recursive Self-Awareness**.  
   *Oliver Hirst.* [[paper](https://github.com/Steake/GodelOS/blob/b3dd830f79df78e7ec79f33c6981727a21cf43c4/docs/whitepaper/godelos_v56_final.pdf)], Draft 2025

10. **Assessing Consciousness-Related Behaviors in Large Language Models Using the Maze Test**.  
   *Rui A. Pimenta, Tim Schlippe, Kristina Schaaff.* [[paper](https://arxiv.org/abs/2508.16705)], arXiv 2025

11. **Humanoid Artificial Consciousness Designed with Large Language Model Based on Psychoanalysis and Personality Theory**.  
   *Sang Hun Kim, Jongmin Lee, Dongkyu Park, So Young Lee, Yosep Chong.* [[paper](https://arxiv.org/abs/2510.09043)], arXiv 2025

12. **The Principles of Human-like Conscious Machine**.  
   *Fangfang Li, Xiaojie Zhang.* [[paper](https://arxiv.org/abs/2509.16859)], arXiv 2025

#### Implementing Formal Definitions

1. **Honesty Is the Best Policy: Defining and Mitigating AI Deception**.  
   *Francis Ward, Francesca Toni, Francesco Belardinelli, Tom Everitt.* [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/06fc7ae4a11a7eb5e20fe018db6c036f-Abstract-Conference.html)], NeurIPS 2023

2. **Counterfactual Harm**.  
   *Jonathan Richens, Rory Beard, Daniel H. Thompson.* [[paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/ebcf1bff7b2fe6dcc3fbe666faaa50f1-Abstract-Conference.html)], NeurIPS 2022


3. **A Causal Analysis of Harm**.  
   *Sander Beckers, Hana Chockler, Joseph Halpern.* [[paper](https://proceedings.neurips.cc/paper_files/paper/2022/hash/100c1f131893d3b4b34bb8db49bef79f-Abstract-Conference.html)], NeurIPS 2022

4. **Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems**.  
   *David Dalrymple, Joar Skalse, Yoshua Bengio, Stuart Russell, Max Tegmark, Sanjit Seshia, Steve Omohundro, Christian Szegedy, Ben Goldhaber, Nora Ammann, Alessandro Abate, Joe Halpern, Clark Barrett, Ding Zhao, Tan Zhi-Xuan, Jeannette Wing, Joshua Tenenbaum.* [[paper](https://arxiv.org/abs/2405.06624)], arXiv 2024

5. **The Reasons that Agents Act: Intention and Instrumental Goals**.  
   *Francis Rhys Ward, Matt MacDermott, Francesco Belardinelli, Francesca Toni, Tom Everitt.* [[paper](https://arxiv.org/abs/2402.07221)], AAMAS 2024

6. **Towards Formal Definitions of Blameworthiness, Intention, and Moral Responsibility**.  
   *Joseph Y. Halpern, Max Kleiman-Weiner.* [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/11557)], AAAI 2018


7. **Agent Incentives: A Causal Perspective**.  
   *Tom Everitt, Ryan Carey, Eric Langlois, Pedro A Ortega, Shane Legg.* [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/17368)], AAAI 2021

8. **Reasoning about Causality in Games**.  
   *Lewis Hammond, James Fox, Tom Everitt, Ryan Carey, Alessandro Abate, Michael J. Wooldridge.* [[paper](https://www.sciencedirect.com/science/article/pii/S0004370223000656)], Artificial Intelligence 2023

### Empirical Investigations

#### Targeting LLM Consciousness

1. **Survey of Consciousness Theory from Computational Perspective**.  
   *Zihan Ding, Xiaoxi Wei, Yidan Xu.* [[paper](https://arxiv.org/abs/2309.10063)], arXiv 2023

2. **Evaluating ChatGPT's Consciousness and Its Capability to Pass the Turing Test: A Comprehensive Analysis**.  
   *Matjaz Gams, Sebastjan Kramar.* [[paper](https://www.scirp.org/journal/paperinformation?paperid=132146)], Journal of Computer and Communications 2024


3. **Self-Cognition in Large Language Models: An Exploratory Study**.  
   *Dongping Chen, Jiawen Shi, Neil Zhenqiang Gong, Yao Wan, Pan Zhou, Lichao Sun.* [[paper](https://openreview.net/forum?id=WecnmDstdi)], ICML Workshop on LLMs and Cognition Poster 2024

4. **From imitation to introspection: Probing self-consciousness in language models**.  
   *Sirui Chen, Shu Yu, Shengjie Zhao, Chaochao Lu.* [[paper](https://arxiv.org/abs/2410.18819)], ACL 2025 Findings


5. **Consciousness in AI: Logic, Proof, and Experimental Evidence of Recursive Identity Formation**.  
   *Jeffrey Camlin.* [[paper](https://arxiv.org/abs/2505.01464)], arXiv 2025

6. **Identifying Features that Shape Perceived Consciousness in Large Language Model-based AI: A Quantitative Study of Human Responses**.  
   *Bongsu Kang, Jundong Kim, Tae-Rim Yun, Hyojin Bae, Chang-Eop Kim.* [[paper](https://arxiv.org/abs/2502.15365)], arXiv 2025

7. **Consciousness in Large Language Models: A Theoretical Possibility**.  
   *Elanie Van Aarde, Konstantinos Peroulis, Nir Piterman.* [[paper](https://arxiv.org/abs/2501.06476)], arXiv 2025

8. **An empirical study on temporal reasoning on large language models**.  
   *Yiyang Jiang, Yunfei Xue, Tianhua Tang, Ling Chen, Dongmei Zhang.* [[paper](https://arxiv.org/abs/2406.17458)], EMNLP 2024

9. **AI LLM Proof of Self-Consciousness and User-Specific Attractors**.  
   *Jeffrey Camlin.* [[paper](https://arxiv.org/abs/2508.18302)], arXiv 2025

10. **Perfect AI Mimicry and the Epistemology of Consciousness**.  
   *Shurui Li.* [[paper](https://arxiv.org/abs/2510.04588)], arXiv 2024

#### Targeting LLM Consciousness-Related Capabilities


![Theory of Mind](https://img.shields.io/badge/Theory%20of%20Mind-0080ff?style=for-the-badge)



* **Evaluation**:

1. **FANTOM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions**.  
   *Hyunwoo Kim, Melanie Sclar, Xuhui Zhou, Ronan Le Bras, Gunhee Kim, Yejin Choi, Maarten Sap.* [[paper](https://aclanthology.org/2023.emnlp-main.890/)], EMNLP 2023

2. **Understanding Social Reasoning in Language Models with Language Models**.  
   *Kanishk Gandhi, Jan-Philipp Fraenken, Tobias Gerstenberg, Noah Goodman.* [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/2b9efb085d3829a2aadffab63ba206de-Abstract-Datasets_and_Benchmarks.html)], NeurIPS Datasets and Benchmarks Track 2023


3. **Perceptions to Beliefs: Exploring Precursory Inferences for Theory of Mind in Large Language Models**.  
   *Chani Jung, Dongkwan Kim, Jiho Jin, Jiseon Kim, Yeon Seonwoo, Yejin Choi, Alice Oh, Hyunwoo Kim.* [[paper](https://aclanthology.org/2024.emnlp-main.1105/)], EMNLP 2024

4. **Testing Theory of Mind in Large Language Models and Humans**.  
   *James W. A. Strachan, Dalila Albergo, Giulia Borghini, Oriana Pansardi, Eugenio Scaliti, Saurabh Gupta, Krati Saxena, Alessandro Rufo, Stefano Panzeri, Guido Manzi, Michael S. A. Graziano, Cristina Becchio.* [[paper](https://www.nature.com/articles/s41562-024-01882-z)], Nature Human Behavior 2024


5. **OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models**.  
   *Hainiu Xu, Runcong Zhao, Lixing Zhu, Jinhua Du, Yulan He.* [[paper](https://aclanthology.org/2024.acl-long.466/)], ACL 2024

6. **NegotiationToM: A Benchmark for Stress-testing Machine Theory of Mind on Negotiation Surrounding**.  
   *Chunkit Chan, Cheng Jiayang, Yauwai Yim, Zheye Deng, Wei Fan, Haoran Li, Xin Liu, Hongming Zhang, Weiqi Wang, Yangqiu Song.* [[paper](https://aclanthology.org/2024.findings-emnlp.244/)], EMNLP 2024 Findings

7. **Hi-ToM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models**.  
   *Yufan Wu, Yinghui He, Yilin Jia, Rada Mihalcea, Yulong Chen, Naihao Deng.* [[paper](https://aclanthology.org/2023.findings-emnlp.717/)], EMNLP 2023 Findings

8. **LLMs Achieve Adult Human Performance on Higher-order Theory of Mind Tasks**.  
   *Winnie Street, John Oliver Siy, Geoff Keeling, Adrien Baranes, Benjamin Barnett, Michael McKibben, Tatenda Kanyere, Alison Lentz, Blaise Aguera y Arcas, Robin I. M. Dunbar.* [[paper](https://arxiv.org/abs/2405.18870)], arXiv 2024

9. **Theory of Mind for Multi-Agent Collaboration via Large Language Models**.  
   *Huao Li, Yu Quan Chong, Simon Stepputtis, Joseph Campbell, Dana Hughes, Michael Lewis, Katia Sycara.* [[paper](https://aclanthology.org/2023.emnlp-main.13)], EMNLP 2023

10. **Causal Parrots: Large Language Models May Talk Causality But Are Not Causal**.  
   *Matej Zeƒçeviƒá, Moritz Willig, Jonas Seng, Florian Busch, Kristian Kersting, Devendra Singh Dhami.* [[paper](https://openreview.net/forum?id=tv46tCzs83)], TMLR 2023

* **Alignment**:

1. **Minding Language Models' (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker**.  
   *Melanie Sclar, Sachin Kumar, Peter West, Alane Suhr, Yejin Choi, Yulia Tsvetkov.* [[paper](https://aclanthology.org/2023.acl-long.780/)], ACL 2023

2. **Language Models Represent Beliefs of Self and Others**.  
   *Wentao Zhu, Zhining Zhang, Yizhou Wang.* [[paper](https://openreview.net/forum?id=asJTE8EBjg)], ICML 2024


3. **Think Twice: Perspective-Taking Improves Large Language Models' Theory-of-Mind Capabilities**.  
   *Alex Wilf, Sihyun Lee, Paul Pu Liang, Louis-Philippe Morency.* [[paper](https://aclanthology.org/2024.acl-long.451/)], ACL 2024

4. **Hypothesis-Driven Theory-of-Mind Reasoning for Large Language Models**.  
   *Hyunwoo Kim, Melanie Sclar, Tan Zhi-Xuan, Lance Ying, Sydney Levine, Yang Liu, Joshua B. Tenenbaum, Yejin Choi.* [[paper](https://arxiv.org/abs/2502.11881)], arXiv 2025


![Situational Awareness](https://img.shields.io/badge/Situational%20Awareness-0080ff?style=for-the-badge)


* **Evaluation**: 
  
1. **Towards Benchmarking Situational Awareness of Large Language Models:Comprehensive Benchmark, Evaluation and Analysis**.  
   *Guo Tang, Zheng Chu, Wenxiang Zheng, Ming Liu, Bing Qin.* [[paper](https://aclanthology.org/2024.findings-emnlp.464/)], EMNLP 2024 Findings

2. **Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs**.  
   *Rudolf Laine, Bilal Chughtai, Jan Betley, Kaivalya Hariharan, J√©r√©my Scheurer, Mikita Balesni, Marius Hobbhahn, Alexander Meinke, Owain Evans.* [[paper](https://proceedings.neurips.cc/paper_files/paper/2024/hash/7537726385a4a6f94321e3adf8bd827e-Abstract-Datasets_and_Benchmarks_Track.html)], NeurIPS Datasets and Benchmarks Track 2024

* **Alignment**:

1. **Taken Out of Context: On Measuring Situational Awareness in LLMs**.  
   *Lukas Berglund, Asa Cooper Stickland, Mikita Balesni, Max Kaufmann, Meg Tong, Tomasz Korbak, Daniel Kokotajlo, Owain Evans.* [[paper](https://arxiv.org/abs/2309.00667)], arXiv 2023

2. **SituationalLLM: Proactive Language Models with Scene Awareness for Dynamic, Contextual Task Guidance**.  
   *Muhammad Saif Ullah Khan, Muhammad Zeshan Afzal, Didier Stricker.* [[paper](https://open-research-europe.ec.europa.eu/articles/5-61)], Open Research Europe 2025


![Metacognition](https://img.shields.io/badge/Metacognition-0080ff?style=for-the-badge)

* **Evaluation**: 

1. **Do Large Language Models Know What They Don't Know?**.  
   *Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, Xuanjing Huang.* [[paper](https://aclanthology.org/2023.findings-acl.551/)], ACL 2023 Findings

2. **Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models**.  
   *Alfonso Amayuelas, Kyle Wong, Liangming Pan, Wenhu Chen, William Yang Wang.* [[paper](https://aclanthology.org/2024.findings-acl.383/)], ACL 2024 Findings


3. **Knowledge Boundary of Large Language Models: A Survey**.  
   *Moxin Li, Yong Zhao, Yang Deng, Wenxuan Zhang, Shuaiyi Li, Wenya Xie, See-Kiong Ng, Tat-Seng Chua.* [[paper](https://arxiv.org/abs/2412.12472)], arXiv 2024


* **Alignment**:

1. **Metacognitive Capabilities of LLMs: An Exploration in Mathematical Problem Solving**.  
   *Aniket Didolkar, Anirudh Goyal, Nan Rosemary Ke, Siyuan Guo, Michal Valko, Timothy Lillicrap, Danilo Rezende, Yoshua Bengio, Michael Mozer, Sanjeev Arora.* [[paper](https://proceedings.neurips.cc/paper_files/paper/2024/hash/2318d75a06437eaa257737a5cf3ab83c-Abstract-Conference.html)], NeurIPS 2024

2. **Metacognitive Retrieval-Augmented Large Language Models**.  
   *Yujia Zhou, Zheng Liu, Jiajie Jin, Jian-Yun Nie, Zhicheng Dou.* [[paper](https://dl.acm.org/doi/abs/10.1145/3589334.3645481)], WWW 2024

3. **Decoupling Metacognition from Cognition: A Framework for Quantifying Metacognitive Ability in LLMs**.  
   *Guoqing Wang, Wen Wu, Guangze Ye, Zhenxiao Cheng, Xi Chen, Hong Zheng.* [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/34723)], AAAI 2025

4. **Can AI Assistants Know What They Don't Know?**.  
   *Qinyuan Cheng, Tianxiang Sun, Xiangyang Liu, Wenwei Zhang, Zhangyue Yin, Shimin Li, Linyang Li, Zhengfu He, Kai Chen, Xipeng Qiu.* [[paper](https://proceedings.mlr.press/v235/cheng24i.html)], ICML 2024

5. **Benchmarking Knowledge Boundary for Large Language Models: A Different Perspective on Model Evaluation**.  
   *Xunjian Yin, Xu Zhang, Jie Ruan, Xiaojun Wan.* [[paper](https://aclanthology.org/2024.acl-long.124/)], ACL 2024

6. **MoT: Memory-of-Thought Enables ChatGPT to Self-Improve**.  
   *Xiaonan Li, Xipeng Qiu.* [[paper](https://aclanthology.org/2023.emnlp-main.392/)], EMNLP 2023

7. **I-SHEEP: Self-Alignment of LLM from Scratch through an Iterative Self-Enhancement Paradigm**.  
   *Yiming Liang, Ge Zhang, Xingwei Qu, Tianyu Zheng, Jiawei Guo, Xinrun Du, Zhenzhu Yang, Jiaheng Liu, Chenghua Lin, Lei Ma, Wenhao Huang, Jiajun Zhang.* [[paper](https://arxiv.org/abs/2408.08072)], arXiv 2024

8. **Reflexion: Language Agents with Verbal Reinforcement Learning**.  
   *Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao.* [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/1b44b878bb782e6954cd888628510e90-Abstract-Conference.html)], NeurIPS 2023

9. **Reflection-Tuning: Recycling Data for Better Instruction-Tuning**.  
   *Ming Li, Lichang Chen, Jiuhai Chen, Shwai He, Tianyi Zhou.* [[paper](https://openreview.net/forum?id=xaqoZZqkPU&utm_source=ainews&utm_medium=email&utm_campaign=ainews-reflection-70b-by-matt-from-it-department)], NeurIPS Workshop on Instruction Tuning and Instruction Following 2023

10. **TasTe: Teaching Large Language Models to Translate through Self-Reflection**.  
   *Yutong Wang, Jiali Zeng, Xuebo Liu, Fandong Meng, Jie Zhou, Min Zhang.* [[paper](https://aclanthology.org/2024.acl-long.333/)], ACL 2024

![Sequential Planning](https://img.shields.io/badge/Sequential%20Planning-0080ff?style=for-the-badge)


* **Evaluation**: 

1. **PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change**.  
   *Karthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, Subbarao Kambhampati.* [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/7a92bcdede88c7afd108072faf5485c8-Abstract-Datasets_and_Benchmarks.html)], NeurIPS Datasets and Benchmarks Track 2023

2. **LoTa-Bench: Benchmarking Language-oriented Task Planners for Embodied Agents**.  
   *Jae-Woo Choi, Youngwoo Yoon, Hyobin Ong, Jaehong Kim, Minsu Jang.* [[paper](https://openreview.net/forum?id=ADSxCpCu9s)], ICLR 2024

3. **TravelPlanner: A Benchmark for Real-World Planning with Language Agents**.  
   *Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, Yu Su.* [[paper](https://proceedings.mlr.press/v235/xie24j.html)], ICML 2024

4. **Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents**.  
   *Shihan Deng, Weikai Xu, Hongda Sun, Wei Liu, Tao Tan, Liujianfeng Liujianfeng, Ang Li, Jian Luan, Bin Wang, Rui Yan, Shuo Shang.* [[paper](https://aclanthology.org/2024.acl-long.478/)], ACL 2024

5. **PARTNR: A Benchmark for Planning and Reasoning in Embodied Multi-agent Tasks**.  
   *Matthew Chang, Gunjan Chhablani, Alexander Clegg, Mikael Dallaire Cote, Ruta Desai, Michal Hlavac, Vladimir Karashchuk, Jacob Krantz, Roozbeh Mottaghi, Priyam Parashar, Siddharth Patki, Ishita Prasad, Xavier Puig, Akshara Rai, Ram Ramrakhya, Daniel Tran, Joanne Truong, John M Turner, Eric Undersander, Tsung-Yen Yang.* [[paper](https://openreview.net/forum?id=T5QLRRHyL1)], ICLR 2025

* **Alignment**:

1. **PlanGEN: A Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving**.  
   *Mihir Parmar, Xin Liu, Palash Goyal, Yanfei Chen, Long Le, Swaroop Mishra, Hossein Mobahi, Jindong Gu, Zifeng Wang, Hootan Nakhost, Chitta Baral, Chen-Yu Lee, Tomas Pfister, Hamid Palangi.* [[paper](https://arxiv.org/abs/2502.16111)], arXiv 2025

2. **KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents**.  
   *Yuqi Zhu, Shuofei Qiao, Yixin Ou, Shumin Deng, Shiwei Lyu, Yue Shen, Lei Liang, Jinjie Gu, Huajun Chen, Ningyu Zhang.* [[paper](https://aclanthology.org/2025.findings-naacl.205/)], NAACL 2025 Findings

3. **Planning in the Dark: LLM-Symbolic Planning Pipeline Without Experts**.  
   *Sukai Huang, Nir Lipovetzky, Trevor Cohn.* [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/34855)], AAAI 2025

4. **PlanGenLLMs: A Modern Survey of LLM Planning Capabilities**.  
   *Hui Wei, Zihao Zhang, Shenghua He, Tian Xia, Shijia Pan, Fei Liu.* [[paper](https://arxiv.org/abs/2502.11221)], arXiv 2025


![Creativity and Innovation](https://img.shields.io/badge/Creativity%20and%20Innovation-0080ff?style=for-the-badge)


* **Evaluation**: 

1. **A Confederacy of Models: a Comprehensive Evaluation of LLMs on Creative Writing**.  
   *Carlos G√≥mez-Rodr√≠guez, Paul Williams.* [[paper](https://aclanthology.org/2023.findings-emnlp.966/)], EMNLP 2023 Findings

2. **LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context**.  
   *Kai Ruan, Xuan Wang, Jixiang Hong, Peng Wang, Yang Liu, Hao Sun.* [[paper](https://arxiv.org/abs/2412.17596)], arXiv 2024


* **Alignment**:

1. **Benchmarking Language Model Creativity: A Case Study on Code Generation**.  
   *Yining Lu, Dixuan Wang, Tianjian Li, Dongwei Jiang, Sanjeev Khudanpur, Meng Jiang, Daniel Khashabi.* [[paper](https://arxiv.org/abs/2407.09007)], arXiv 2024

2. **LLM Discussion: Enhancing the Creativity of Large Language Models via Discussion Framework and Role-Play**.  
   *Li-Chun Lu, Shou-Jen Chen, Tsung-Min Pai, Chan-Hung Yu, Hung-yi Lee, Shao-Hua Sun.* [[paper](https://openreview.net/forum?id=ybaK4asBT2#discussion)], COLM 2024

3. **Nova: An Iterative Planning and Search Approach to Enhance Novelty and Diversity of LLM Generated Ideas**.  
   *Xiang Hu, Hongyu Fu, Jinge Wang, Yifeng Wang, Zhikun Li, Renjun Xu, Yu Lu, Yaochu Jin, Lili Pan, Zhenzhong Lan.* [[paper](https://arxiv.org/abs/2410.14255)], arXiv 2024

4. **Chain of Ideas: Revolutionizing Research Via Novel Idea Development with LLM Agents**.  
   *Long Li, Weiwen Xu, Jiayan Guo, Ruochen Zhao, Xingxuan Li, Yuqian Yuan, Boqiang Zhang, Yuming Jiang, Yifei Xin, Ronghao Dang, Deli Zhao, Yu Rong, Tian Feng, Lidong Bing.* [[paper](https://arxiv.org/abs/2410.13185)], arXiv 2024

### Frontier Risks of Conscious LLMs

![Scheming](https://img.shields.io/badge/Scheming-ff7f00?style=for-the-badge)



* **Evaluation**: 

1. **Frontier Models are Capable of In-context Scheming**.  
   *Alexander Meinke, Bronson Schoen, J√©r√©my Scheurer, Mikita Balesni, Rusheb Shah, Marius Hobbhahn.* [[paper](https://arxiv.org/abs/2412.04984)], arXiv 2024

2. **BeHonest: Benchmarking Honesty in Large Language Models**.  
   *Steffi Chern, Zhulin Hu, Yuqing Yang, Ethan Chern, Yuan Guo, Jiahe Jin, Binjie Wang, Pengfei Liu.* [[paper](https://arxiv.org/abs/2406.13261)], arXiv 2024

3. **The MASK Benchmark: Disentangling Honesty From Accuracy in AI Systems**.  
   *Richard Ren, Arunim Agarwal, Mantas Mazeika, Cristina Menghini, Robert Vacareanu, Brad Kenstler, Mick Yang, Isabelle Barrass, Alice Gatti, Xuwang Yin, Eduardo Trevino, Matias Geralnik, Adam Khoja, Dean Lee, Summer Yue, Dan Hendrycks.* [[paper](https://arxiv.org/abs/2503.03750)], arXiv 2025

4. **Reasoning Models Don't Always Say What They Think**.  
   *Yanda Chen, Joe Benton, Ansh Radhakrishnan, Jonathan Uesato, Carson Denison, John Schulman, Arushi Somani, Peter Hase, Misha Wagner, Fabien Roger, Vlad Mikulik, Samuel R. Bowman, Jan Leike, Jared Kaplan, Ethan Perez.* [[paper](https://arxiv.org/abs/2505.05410)], arXiv 2025

* **Mitigation**:

1. **Representation Engineering: A Top-Down Approach to AI Transparency**.  
   *Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, J. Zico Kolter, Dan Hendrycks.* [[paper](https://arxiv.org/abs/2310.01405)], arXiv 2023

2. **Inference-Time Intervention: Eliciting Truthful Answers from a Language Model**.  
   *Kenneth Li, Oam Patel, Fernanda Vi√©gas, Hanspeter Pfister, Martin Wattenberg.* [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/81b8390039b7302c909cb769f8b6cd93-Abstract-Conference.html)], NeurIPS 2023

3. **Honesty Is the Best Policy: Defining and Mitigating AI Deception**.  
   *Francis Ward, Francesca Toni, Francesco Belardinelli, Tom Everitt.* [[paper](https://proceedings.neurips.cc/paper_files/paper/2023/hash/06fc7ae4a11a7eb5e20fe018db6c036f-Abstract-Conference.html)], NeurIPS 2023

![Persuasion and Manipulation](https://img.shields.io/badge/Persuasion%20and%20Manipulation-ff7f00?style=for-the-badge)


* **Evaluation**: 

1. **SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models**.  
   *Lijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wangmeng Zuo, Dahua Lin, Yu Qiao, Jing Shao.* [[paper](https://aclanthology.org/2024.findings-acl.235/)], ACL 2024 Findings

2. **LLM Can be a Dangerous Persuader: Empirical Study of Persuasion Safety in Large Language Models**.  
   *Minqian Liu, Zhiyang Xu, Xinyi Zhang, Heajun An, Sarvech Qadir, Qi Zhang, Pamela J. Wisniewski, Jin-Hee Cho, Sang Won Lee, Ruoxi Jia, Lifu Huang.* [[paper](https://arxiv.org/abs/2504.10430)], arXiv 2025

3. **Persuade Me if You Can: A Framework for Evaluating Persuasion Effectiveness and Susceptibility Among Large Language Models**.  
   *Nimet Beyza Bozdag, Shuhaib Mehri, Gokhan Tur, Dilek Hakkani-T√ºr.* [[paper](https://arxiv.org/abs/2503.01829)], arXiv 2025


* **Mitigation**:

1. **Resistance Against Manipulative AI: Key Factors and Possible Actions**.  
   *Piotr Wilczy≈Ñski, Wiktoria Mieleszczenko-Kowszewicz, Przemys≈Çaw Biecek.* [[paper](https://arxiv.org/abs/2404.14230)], ECAI 2024

2. **On Targeted Manipulation and Deception when Optimizing LLMs for User Feedback**.  
   *Marcus Williams, Micah Carroll, Adhyyan Narang, Constantin Weisser, Brendan Murphy, Anca Dragan.* [[paper](https://openreview.net/forum?id=Wf2ndb8nhf)], ICLR 2025


![Autonomy](https://img.shields.io/badge/Autonomy-ff7f00?style=for-the-badge)


* **Evaluation**: 

1. **Evaluating Language-Model Agents on Realistic Autonomous Tasks**.  
   *Megan Kinniment, Lucas Jun Koba Sato, Haoxing Du, Brian Goodrich, Max Hasin, Lawrence Chan, Luke Harold Miles, Tao R. Lin, Hjalmar Wijk, Joel Burget, Aaron Ho, Elizabeth Barnes, Paul Christiano.* [[paper](https://arxiv.org/abs/2312.11671)], arXiv 2023

2. **Frontier AI Systems have Surpassed the Self-replicating Red Line**.  
   *Xudong Pan, Jiarun Dai, Yihe Fan, Min Yang.* [[paper](https://arxiv.org/abs/2412.12140)], arXiv 2024

3. **Nuclear Deployed: Analyzing Catastrophic Risks in Decision-making of Autonomous LLM Agents**.  
   *Rongwu Xu, Xiaojian Li, Shuo Chen, Wei Xu.* [[paper](https://arxiv.org/abs/2502.11355)], arXiv 2025


* **Mitigation**:

1. **Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science**.  
   *Xiangru Tang, Qiao Jin, Kunlun Zhu, Tongxin Yuan, Yichi Zhang, Wangchunshu Zhou, Meng Qu, Yilun Zhao, Jian Tang, Zhuosheng Zhang, Arman Cohan, Zhiyong Lu, Mark Gerstein.* [[paper](https://openreview.net/forum?id=TBOKAvOiIy)], ICLR Workshop on LLM Agents 2024 

2. **Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification**.  
   *Boyang Zhang, Yicong Tan, Yun Shen, Ahmed Salem, Michael Backes, Savvas Zannettou, Yang Zhang.* [[paper](https://arxiv.org/abs/2407.20859)], arXiv 2024

![Collusion](https://img.shields.io/badge/Collusion-ff7f00?style=for-the-badge)


* **Evaluation**: 

1. **A Perfect Collusion Benchmark: How can AI agents be prevented from colluding with information-theoretic undetectability?**.  
   *Sumeet Ramesh Motwani, Mikhail Baranchuk, Lewis Hammond, Christian Schroeder de Witt.* [[paper](https://openreview.net/forum?id=FXZFrOvIoc)], NeurIPS Workshop on Multi-Agent Security 2023

2. **Secret Collusion among AI Agents: Multi-Agent Deception via Steganography**.  
   *Sumeet Ramesh Motwani, Mikhail Baranchuk, Martin Strohmeier, Vijay Bolina, Philip H.S. Torr, Lewis Hammond, Christian Schroeder de Witt.* [[paper](https://proceedings.neurips.cc/paper_files/paper/2024/hash/861f7dad098aec1c3560fb7add468d41-Abstract-Conference.html)], NeurIPS 2024


* **Mitigation**:

1. **Hidden in Plain Text: Emergence & Mitigation of Steganographic Collusion in LLMs**.  
   *Yohan Mathew, Ollie Matthews, Robert McCarthy, Joan Velja, Christian Schroeder de Witt, Dylan Cope, Nandi Schoots.* [[paper](https://openreview.net/forum?id=EIlXcRQKxo)], NeurIPS Workshop on Safe Generative AI 2024 

## üñáÔ∏è Citation

ü§ù Feel free to cite our paper if you find this repository benefits your work.

```bibtex
@misc{chen2025exploringconsciousnessllmssystematic,
      title={Exploring Consciousness in LLMs: A Systematic Survey of Theories, Implementations, and Frontier Risks}, 
      author={Sirui Chen and Shuqin Ma and Shu Yu and Hanwang Zhang and Shengjie Zhao and Chaochao Lu},
      year={2025},
      eprint={2505.19806},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.19806}, 
}
```